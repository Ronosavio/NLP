{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9010b6c-67f2-41fa-89fb-a9a70ce3cedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4729c591-053f-40ec-9a83-b9be5bf0f38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wikitext-2/wiki.train.tokens', 'r', encoding = 'utf8') as trd:\n",
    "    train_data = trd.read()\n",
    "with open('D:/ds/NLP/files_for_nlp/wiki.valid.tokens', 'r', encoding = 'utf8') as vd:\n",
    "    valid_data = vd.read()\n",
    "with open('D:/ds/NLP/files_for_nlp/wiki.test.tokens', 'r', encoding = 'utf8') as td:\n",
    "    test_data = td.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a383692b-45d2-4743-9ba7-ee55cc0eac34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " = Valkyria Chronicles III = \n",
      " \n",
      " Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \n",
      " The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more <unk> for series newcomers . Char\n"
     ]
    }
   ],
   "source": [
    "print(train_data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9b522eb-6926-4fcf-8385-2cd427396619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'are', 'doing', 'good', 'good,', 'how', 'in', 'life,', 'we', 'you?'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 'we are doing good, in life, how good are you?'\n",
    "set(x.split())  # not a good idea for tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74b68f96-6b75-4e9d-be49-a6b08a585260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -U 'spacy[apple]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19082f53-67a3-43cf-8119-67ca6a3ca51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we\n",
      "are\n",
      "doing\n",
      "good\n",
      ",\n",
      "in\n",
      "life\n",
      ",\n",
      "how\n",
      "good\n",
      "are\n",
      "you\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# !python3 -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(x)\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d783603-c953-49af-95f7-6a0a4030e4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def token_generator(text:str)->List[str]:\n",
    "    try:\n",
    "        assert type(text) == str\n",
    "        doc = nlp(text)\n",
    "        tokens = [token.text for token in doc]\n",
    "        return tokens\n",
    "    \n",
    "    except AssertionError:\n",
    "        print('input should be a string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcf42018-2b9b-430a-85e5-0960f2962cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = token_generator(train_data[:1000000])\n",
    "test_tokens = token_generator(test_data[:100000])\n",
    "valid_tokens = token_generator(valid_data[:100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6932d72f-5d42-42da-96f0-c2969348b00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' \\n ', '=', 'Valkyria', 'Chronicles', 'III', '=', '\\n \\n ', 'Senjō', 'no', 'Valkyria', '3', ':', '<', 'unk', '>', 'Chronicles', '(', 'Japanese', ':', '戦場のヴァルキュリア3', ',', 'lit', '.', 'Valkyria', 'of', 'the', 'Battlefield', '3', ')', ',', 'commonly', 'referred', 'to', 'as', 'Valkyria', 'Chronicles', 'III', 'outside', 'Japan', ',', 'is', 'a', 'tactical', 'role', '@-@', 'playing', 'video', 'game', 'developed', 'by', 'Sega', 'and', 'Media', '.', 'Vision', 'for', 'the', 'PlayStation', 'Portable', '.', 'Released', 'in', 'January', '2011', 'in', 'Japan', ',', 'it', 'is', 'the', 'third', 'game', 'in', 'the', 'Valkyria', 'series', '.', '<', 'unk', '>', 'the', 'same', 'fusion', 'of', 'tactical', 'and', 'real', '@-@', 'time', 'gameplay', 'as', 'its', 'predecessors', ',', 'the', 'story', 'runs', 'parallel', 'to', 'the']\n"
     ]
    }
   ],
   "source": [
    "print(train_tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "870483b5-ec52-4bf2-b51f-7e58d35eacfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install english-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db3610ba-3193-41fa-b247-d704774b034b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import english_words\n",
    "words = english_words.get_english_words_set(['web2'], lower = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8512eee9-f5bb-4b39-9177-ffde31fc2121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def data_cleaning_tokens(data: List[str], words:set)->List[str]:\n",
    "    '''\n",
    "    Apply the below clearning methods:\n",
    "    1. Lower case all the tokens.\n",
    "    2. Check if the token is part of english dictionary.\n",
    "    3. Remove all the special characters.\n",
    "    4. Remove all the greek letters\n",
    "    5. Remove the digits.\n",
    "    \n",
    "    '''\n",
    "    data = [token.lower() for token in data]\n",
    "    data = [token for token in data if token in words]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e35d75e-fd8f-437b-a229-2e1e2d00ef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = data_cleaning_tokens(train_tokens, words)\n",
    "test_tokens = data_cleaning_tokens(test_tokens, words)\n",
    "valid_tokens = data_cleaning_tokens(valid_tokens, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4569734-e8a4-4f79-a4ac-72928803493d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125696\n",
      "12247\n",
      "12511\n"
     ]
    }
   ],
   "source": [
    "print(len(train_tokens))\n",
    "print(len(valid_tokens))\n",
    "print(len(test_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c9a823b-2450-4ec5-b6ee-b4cecff60e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def unique_vocab(tokens):\n",
    "    unique = [i for i, j in Counter(tokens).most_common()]\n",
    "    char_to_num = {j : i for i, j in enumerate(unique)}\n",
    "    num_to_char = {j : i for i, j in char_to_num.items()}\n",
    "    return char_to_num, num_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e039b65-9fc1-4f32-b565-d87074602793",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_char_to_num_dict, train_num_to_char_dict = unique_vocab(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df009172-ff68-4ab0-91fe-c4cb2243bd00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[284, 71, 284, 1690, 2131, 284, 1, 0, 1691, 2132]\n"
     ]
    }
   ],
   "source": [
    "# convert text data to numerical data\n",
    "tt = [train_char_to_num_dict[i] for i in train_tokens]\n",
    "vt = [train_char_to_num_dict[i] for i in valid_tokens if i in train_char_to_num_dict]\n",
    "ttt = [train_char_to_num_dict[i] for i in test_tokens if i in train_char_to_num_dict]\n",
    "print(tt[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ae4bc2f-f8a4-4df9-8e26-583df696d6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the tt 125696\n",
      "Words each RNNs copy will be process: 7856\n",
      "how many seq will be processed 246\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "seq_len = 32\n",
    "print(f'length of the tt {len(tt)}')\n",
    "print(f'Words each RNNs copy will be process: {round(len(tt)/batch_size)}')\n",
    "print(f'how many seq will be processed {round(round(len(tt)/batch_size)/seq_len)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78be532a-13af-4a2f-bf16-1c4596ec474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    arr = np.asarray(arr)\n",
    "    batch_size_total = batch_size * seq_length\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "78c0ce8b-eed7-4107-b19d-076e7829caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = get_batches(tt, batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f9293631-25a0-422f-a16f-62694b4a4255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******  X  *********\n",
      "\n",
      "[[  19   14    0  187   55    3    0  284  125    0  144 4051    1 3280\n",
      "     2  501   45    7   37    0  582 2401    4    0   33   55    2    0\n",
      "  1218    5 4052  203]\n",
      " [   8   19    4    0  136  165    8   27  171    0 3364   91 4171    3\n",
      "   285    2 1025  569 2468    0  165    4    0 1721    1  526  708    7\n",
      "     9    0  967    1]\n",
      " [   0   72    6    5  598    2  325 1993   29  278    0  310   10  260\n",
      "     1    0   60  351    3 1478    3    0 2450 1994 1995  412 4300   29\n",
      "     6 1072    7    5]\n",
      " [   0 1620  538  109   23   11   68   48  391 1751    8    1   31  134\n",
      "     2   68   57 1048  106   11    0 3520    1    3  177  402    7  106\n",
      "     7   11    0 2035]\n",
      " [ 509 5878 2609   14   49 3595    3    0   19    2   14 1505   56   10\n",
      "     3   66 1824  110  317    3    2  374   87   70   24    4 2041 1636\n",
      "   498  176    0 2305]\n",
      " [ 126   27  478    7    5  543 1077  165    4  174    0  182    0 3548\n",
      "    64   24   15 4625 2634    1    0 3675 3404  844  679   24  103    1\n",
      "     0   52  559  204]\n",
      " [  90    1  686   12    0   45   19    6  173   15  457  219  558  379\n",
      "     4  110  873   28 6217    2 6218 1565  317 1663 2897 1664   70   21\n",
      "    40   71 1944 1794]\n",
      " [1496   24 1671 1304    5 2107 3149 1304   16 1187    0 1187 1814    2\n",
      "   192  358  666    0  322  666   14    5  594  824  933 3144   11    9\n",
      "   128    1   37 1978]\n",
      " [  10  715  355   41   84   15    3  155 1048    3  768 2277   17 2913\n",
      "    12    5  492 4921   15    5 1354  129 6573  263  334 1767 2782    5\n",
      "  6574  771    3   75]\n",
      " [6681    6   12  929  492 4921    2   57   65  350 1329    9  570  619\n",
      "     7    0 4962   44  460  183   13  829    3  227    2   39   38   34\n",
      "  1930 3188  536    2]\n",
      " [2037   10    0 3455   91  573    9    0 1611  326    2  280   34    0\n",
      "    33  574  367    0  493  801 2351 1554 6835    6 4646    2 2718   30\n",
      "  5040    0  493 1775]\n",
      " [  15  137   15 1654 2215  505   30    0 2215   11 2224  511  443  145\n",
      "    18    5  725 1868  516   51    5   88 1307  428  835  812    0  812\n",
      "     4    0  163    1]\n",
      " [ 153    2 1086   10   49   86  138   15 2733   12   61   14    0  634\n",
      "   287   72    1   43   45   10 5130    0   72 1098  926   18    0    1\n",
      "     2   14    5  715]\n",
      " [ 998    5 4340    2    5 2984  123  313    8   13  170   32 1807 7293\n",
      "    10   62   53    8 1382   14   53   10   81  313    6   65  675   18\n",
      "  1848 1201 1684  863]\n",
      " [   0  338   15  140    4 3250 7434   22    6    0   33  347    0   21\n",
      "  4003    3    1  387  123 3250   65  113    2    0  490  756    5  763\n",
      "    83   17   90    1]\n",
      " [   2   46   26 1525   13  297  210   36    6 7591   12    0 1151    1\n",
      "    28    3  561   13  255    8    0  238  424  304   15    6    8   21\n",
      "    40    3  281  662]]\n",
      "\n",
      " ---------------------------- \n",
      "\n",
      "*********  y  *********\n",
      "\n",
      "[[  14    0  187   55    3    0  284  125    0  144 4051    1 3280    2\n",
      "   501   45    7   37    0  582 2401    4    0   33   55    2    0 1218\n",
      "     5 4052  203  722]\n",
      " [  19    4    0  136  165    8   27  171    0 3364   91 4171    3  285\n",
      "     2 1025  569 2468    0  165    4    0 1721    1  526  708    7    9\n",
      "     0  967    1 5442]\n",
      " [  72    6    5  598    2  325 1993   29  278    0  310   10  260    1\n",
      "     0   60  351    3 1478    3    0 2450 1994 1995  412 4300   29    6\n",
      "  1072    7    5 1343]\n",
      " [1620  538  109   23   11   68   48  391 1751    8    1   31  134    2\n",
      "    68   57 1048  106   11    0 3520    1    3  177  402    7  106    7\n",
      "    11    0 2035    1]\n",
      " [5878 2609   14   49 3595    3    0   19    2   14 1505   56   10    3\n",
      "    66 1824  110  317    3    2  374   87   70   24    4 2041 1636  498\n",
      "   176    0 2305 3022]\n",
      " [  27  478    7    5  543 1077  165    4  174    0  182    0 3548   64\n",
      "    24   15 4625 2634    1    0 3675 3404  844  679   24  103    1    0\n",
      "    52  559  204   11]\n",
      " [   1  686   12    0   45   19    6  173   15  457  219  558  379    4\n",
      "   110  873   28 6217    2 6218 1565  317 1663 2897 1664   70   21   40\n",
      "    71 1944 1794  182]\n",
      " [  24 1671 1304    5 2107 3149 1304   16 1187    0 1187 1814    2  192\n",
      "   358  666    0  322  666   14    5  594  824  933 3144   11    9  128\n",
      "     1   37 1978  128]\n",
      " [ 715  355   41   84   15    3  155 1048    3  768 2277   17 2913   12\n",
      "     5  492 4921   15    5 1354  129 6573  263  334 1767 2782    5 6574\n",
      "   771    3   75    8]\n",
      " [   6   12  929  492 4921    2   57   65  350 1329    9  570  619    7\n",
      "     0 4962   44  460  183   13  829    3  227    2   39   38   34 1930\n",
      "  3188  536    2  391]\n",
      " [  10    0 3455   91  573    9    0 1611  326    2  280   34    0   33\n",
      "   574  367    0  493  801 2351 1554 6835    6 4646    2 2718   30 5040\n",
      "     0  493 1775  801]\n",
      " [ 137   15 1654 2215  505   30    0 2215   11 2224  511  443  145   18\n",
      "     5  725 1868  516   51    5   88 1307  428  835  812    0  812    4\n",
      "     0  163    1 3154]\n",
      " [   2 1086   10   49   86  138   15 2733   12   61   14    0  634  287\n",
      "    72    1   43   45   10 5130    0   72 1098  926   18    0    1    2\n",
      "    14    5  715    1]\n",
      " [   5 4340    2    5 2984  123  313    8   13  170   32 1807 7293   10\n",
      "    62   53    8 1382   14   53   10   81  313    6   65  675   18 1848\n",
      "  1201 1684  863  745]\n",
      " [ 338   15  140    4 3250 7434   22    6    0   33  347    0   21 4003\n",
      "     3    1  387  123 3250   65  113    2    0  490  756    5  763   83\n",
      "    17   90    1    0]\n",
      " [  46   26 1525   13  297  210   36    6 7591   12    0 1151    1   28\n",
      "     3  561   13  255    8    0  238  424  304   15    6    8   21   40\n",
      "     3  281  662  138]]\n"
     ]
    }
   ],
   "source": [
    "data, label = next(iter(train_batch))\n",
    "print('*******  X  *********\\n')\n",
    "print(data)\n",
    "print('\\n ---------------------------- \\n')\n",
    "print('*********  y  *********\\n')\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45eb03ec-cf17-4b77-9b1a-cad82a9f7500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b97a453d-63e8-4bd0-835e-806c364fd1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class wordRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, embedding_dim = 50, n_hidden=512, n_layers=2,\n",
    "                               drop_prob=0.5):\n",
    "        super().__init__()\n",
    "        self.tokens = tokens\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        # creating word dictionaries\n",
    "        c_tokens = Counter(self.tokens)\n",
    "        self.n_vocab = len(c_tokens)\n",
    "        \n",
    "        # define the embedding layer\n",
    "        self.embedding = nn.Embedding(self.n_vocab, self.embedding_dim)\n",
    "        \n",
    "        # define the LSTM\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, self.n_hidden, self.n_layers, \n",
    "                            dropout=self.drop_prob, batch_first=True)\n",
    "        \n",
    "        # define a dropout layer\n",
    "        self.dropout = nn.Dropout(self.drop_prob)\n",
    "        \n",
    "        # define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(self.n_hidden, self.n_vocab)\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        # pass the x to embedding layer\n",
    "        embed = self.embedding(x)\n",
    "        \n",
    "        # Get the outputs and the new hidden state from the lstm\n",
    "        r_output, hidden = self.lstm(embed, hidden)\n",
    "    \n",
    "        # pass through a dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        ## TODO: put x through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "51bfed5a-ed86-4cfc-9fb2-b34ef49d3988",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = wordRNN(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "19621cc5-1adf-4c0e-a80a-76d4fa3686c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordRNN(\n",
       "  (embedding): Embedding(7795, 50)\n",
       "  (lstm): LSTM(50, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=512, out_features=7795, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3082b62d-2807-468c-82e9-cd7a31a8bf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c56ceba-153d-4a90-8e73-a64844fb980d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is using device : cpu\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "criterion.to(device)\n",
    "print(f'Model is using device : {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2104d899-3c71-44c3-b23d-ac269441fe22",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch : 0\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██▏                                         | 1/20 [00:24<07:47, 24.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity for epoch : 0 = 951.0311279296875\n",
      "Train loss for epoch : 0 = 7.163327314415756\n",
      "Test loss for epoch : 0 = 6.480057981279161\n",
      "starting epoch : 1\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████▍                                       | 2/20 [00:48<07:16, 24.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity for epoch : 1 = 687.529296875\n",
      "Train loss for epoch : 1 = 6.675674691492198\n",
      "Test loss for epoch : 1 = 6.053810649447971\n",
      "starting epoch : 2\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|██████▌                                     | 3/20 [01:13<06:56, 24.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity for epoch : 2 = 549.011962890625\n",
      "Train loss for epoch : 2 = 6.418089380069655\n",
      "Test loss for epoch : 2 = 5.858153661092122\n",
      "starting epoch : 3\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████▊                                   | 4/20 [01:38<06:34, 24.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity for epoch : 3 = 450.23760986328125\n",
      "Train loss for epoch : 3 = 6.186029108203187\n",
      "Test loss for epoch : 3 = 5.731974124908447\n",
      "starting epoch : 4\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|███████████                                 | 5/20 [02:03<06:11, 24.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity for epoch : 4 = 399.5766296386719\n",
      "Train loss for epoch : 4 = 6.010921200927423\n",
      "Test loss for epoch : 4 = 5.665658791859944\n",
      "starting epoch : 5\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████▏                              | 6/20 [02:28<05:48, 24.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity for epoch : 5 = 354.58203125\n",
      "Train loss for epoch : 5 = 5.880581398399508\n",
      "Test loss for epoch : 5 = 5.627924548255073\n",
      "starting epoch : 6\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███████████████▍                            | 7/20 [02:53<05:23, 24.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity for epoch : 6 = 322.3373107910156\n",
      "Train loss for epoch : 6 = 5.766916956220355\n",
      "Test loss for epoch : 6 = 5.589912202623156\n",
      "starting epoch : 7\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████▌                          | 8/20 [03:18<05:00, 25.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity for epoch : 7 = 288.0140075683594\n",
      "Train loss for epoch : 7 = 5.6577132769993375\n",
      "Test loss for epoch : 7 = 5.551817629072401\n",
      "starting epoch : 8\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|███████████████████▊                        | 9/20 [03:43<04:36, 25.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity for epoch : 8 = 260.6958312988281\n",
      "Train loss for epoch : 8 = 5.547250703889496\n",
      "Test loss for epoch : 8 = 5.539453771379259\n",
      "starting epoch : 9\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████▌                     | 10/20 [04:09<04:11, 25.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity for epoch : 9 = 238.74288940429688\n",
      "Train loss for epoch : 9 = 5.44492598942348\n",
      "Test loss for epoch : 9 = 5.524397744072808\n",
      "starting epoch : 10\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|███████████████████████▋                   | 11/20 [04:34<03:46, 25.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity for epoch : 10 = 218.14332580566406\n",
      "Train loss for epoch : 10 = 5.349889925548008\n",
      "Test loss for epoch : 10 = 5.518144607543945\n",
      "starting epoch : 11\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████▊                 | 12/20 [04:59<03:21, 25.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity for epoch : 11 = 199.09532165527344\n",
      "Train loss for epoch : 11 = 5.2674553880886155\n",
      "Test loss for epoch : 11 = 5.533603880140516\n",
      "starting epoch : 12\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|███████████████████████████▉               | 13/20 [05:25<02:57, 25.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity for epoch : 12 = 185.53334045410156\n",
      "Train loss for epoch : 12 = 5.187007942978217\n",
      "Test loss for epoch : 12 = 5.557198312547472\n",
      "starting epoch : 13\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████             | 14/20 [05:50<02:32, 25.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity for epoch : 13 = 172.75453186035156\n",
      "Train loss for epoch : 13 = 5.114154392359208\n",
      "Test loss for epoch : 13 = 5.585489484998915\n",
      "starting epoch : 14\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████████████████████████████████▎          | 15/20 [06:16<02:06, 25.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity for epoch : 14 = 159.0840606689453\n",
      "Train loss for epoch : 14 = 5.036719195696772\n",
      "Test loss for epoch : 14 = 5.586968104044597\n",
      "starting epoch : 15\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████▍        | 16/20 [06:41<01:41, 25.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity for epoch : 15 = 146.7744140625\n",
      "Train loss for epoch : 15 = 4.968069533912503\n",
      "Test loss for epoch : 15 = 5.598110887739393\n",
      "starting epoch : 16\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████████████████████████████████▌      | 17/20 [07:06<01:16, 25.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity for epoch : 16 = 135.21043395996094\n",
      "Train loss for epoch : 16 = 4.901656087563962\n",
      "Test loss for epoch : 16 = 5.615965631273058\n",
      "starting epoch : 17\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████▋    | 18/20 [07:32<00:50, 25.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity for epoch : 17 = 126.79120635986328\n",
      "Train loss for epoch : 17 = 4.830155148798106\n",
      "Test loss for epoch : 17 = 5.626082261403401\n",
      "starting epoch : 18\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|████████████████████████████████████████▊  | 19/20 [07:57<00:25, 25.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity for epoch : 18 = 123.57683563232422\n",
      "Train loss for epoch : 18 = 4.7693882523750775\n",
      "Test loss for epoch : 18 = 5.639853000640869\n",
      "starting epoch : 19\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 20/20 [08:23<00:00, 25.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity for epoch : 19 = 114.54457092285156\n",
      "Train loss for epoch : 19 = 4.707718513449844\n",
      "Test loss for epoch : 19 = 5.6634123590257435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 64\n",
    "seq_len = 32\n",
    "clip = 5\n",
    "training_size = 0.8\n",
    "\n",
    "train_loss_epoch = []\n",
    "validation_loss_epoch = []\n",
    "\n",
    "train_batch_size = len(list(get_batches(tt, batch_size, seq_len)))\n",
    "valid_batch_size = len(list(get_batches(vt, batch_size, seq_len)))\n",
    "test_batch_size = len(list(get_batches(ttt, batch_size, seq_len)))\n",
    "\n",
    "for epoch in tqdm.tqdm(range(epochs)):\n",
    "    \n",
    "    train_batch = get_batches(tt, batch_size, seq_len)\n",
    "    validation_batch = get_batches(vt, batch_size, seq_len)\n",
    "    \n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    training_loss = 0\n",
    "    validation_loss = 0\n",
    "    perplexity_score = 0\n",
    "    \n",
    "    print(f'starting epoch : {epoch}')\n",
    "    print('-------------------------')\n",
    "\n",
    "    batch_count = 0\n",
    "    for words, labels in get_batches(tt, batch_size, seq_len):  \n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        words = torch.from_numpy(words)\n",
    "        labels = torch.from_numpy(labels)\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        model.zero_grad()\n",
    "\n",
    "        logits, hidden = model(words, hidden)\n",
    "        labels = labels.flatten().long()\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_count += 1\n",
    "\n",
    "        training_loss += loss.detach().item()\n",
    "\n",
    "    for words, labels in get_batches(vt, batch_size, seq_len): \n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        words = torch.from_numpy(words)\n",
    "        labels = torch.from_numpy(labels)\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        model.zero_grad()\n",
    "\n",
    "        logits, hidden = model(words, hidden)\n",
    "        labels = labels.flatten().long()\n",
    "        val_loss = criterion(logits, labels)\n",
    "\n",
    "        batch_count += 1\n",
    "        validation_loss += val_loss.detach().item()\n",
    "        perplexity_score += torch.exp(loss)\n",
    "            \n",
    "    train_loss_total = training_loss/train_batch_size\n",
    "    val_loss_total = validation_loss/valid_batch_size\n",
    "            \n",
    "    train_loss_epoch.append(train_loss_total)\n",
    "    validation_loss_epoch.append(val_loss_total)\n",
    "    \n",
    "    print(f'Test perplexity for epoch : {epoch} = {perplexity_score/valid_batch_size}')\n",
    "    print(f'Train loss for epoch : {epoch} = {train_loss_total}')\n",
    "    print(f'Test loss for epoch : {epoch} = {val_loss_total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f27f952-e83c-466d-8b0e-23192824e000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the language model to auto generate mode\n",
    "def predict(model, word, h=None, top_k=None):\n",
    "        word = torch.tensor(train_char_to_num_dict[word])\n",
    "        word = word.view(1, -1)\n",
    "        word = word.to(device)\n",
    "        h = tuple([each.data for each in h])\n",
    "        out, h = model(word, h)\n",
    "        p = nn.functional.softmax(out, dim=1).data\n",
    "        if torch.cuda.is_available():\n",
    "            p = p.cpu()\n",
    "        if top_k is None:\n",
    "            top_word = np.arange(len(model.n_vocab))\n",
    "        else:\n",
    "            p, top_word = p.topk(top_k)\n",
    "            top_word = top_word.numpy().squeeze()\n",
    "        p = p.numpy().squeeze()\n",
    "        word = np.random.choice(top_word, p=p/p.sum())\n",
    "        return train_num_to_char_dict[word], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49f767ae-9b99-4b93-99c2-0f70043a374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, size, first='The world is going', top_k=None):\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    else:\n",
    "        model.cpu()\n",
    "    model.eval() # eval mode\n",
    "    words = [word for word in first.split()]\n",
    "    h = model.init_hidden(1)\n",
    "    for word in first.split():\n",
    "        out, h = predict(model, word, h, top_k=top_k)\n",
    "    words.append(word)\n",
    "    for i in range(size):\n",
    "        out, h = predict(model, word, h, top_k=top_k)\n",
    "        words.append(out)\n",
    "\n",
    "    return ''.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e80b2fac-56ba-4a8d-84ec-39643f3eb290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionforthePlayStationPortablePortable,,,,.and...,andto.to\"and\".,andand...,..,,.\".toto,.,.,and.,..and.and.\",to,\".,and\".andtoandto,..,.\"to,.and\"toto..to..\"\"\".,,,..to..to.to,.,and..\",to,.,,\"and,,.toto,.and.\",.\"\"and.totoand\"..,.,\",..and.\"to,to,\",.andand,to...\"\",,.,tototo\"and\"\",.,,to.\"\"..and..,.,to.and,.and.\"to.\"\".to,.\"..\"and,...toto\",\",,,...,to\".,.,.\"and.,\"toand,and\".,.to,andand\",\",......,,,.\"..,..to.toand.\"toand.to.,,\"and,\"andtoto,and\".andandand.\",,..\".to\",,to,.,\",and,.toandtoto.totoand\"..to..to.,..,,toand.to,,toand\"and,.toto,..toand.\",...\",.to..,\"\".and,.,.and,andand,,.and,.\"...,,.\"andandtoto.toto.and,to...\",andand..and....,to..,\",andtoand,.\"...\"toand,.,...,\"andto.,.,\".,to,.,.,and...to.,.and,to\"....\",,,,and,.toand.......toto,to,,.\",andandtoto.andand...and...,and..,.,.and\",,\"and\",and,.,..to.,,..,,.and.andtoto,toto.\",\",.,..toand\"and,\"andto.andand,,,.to..,to,.and,to,...,..and,,..,\"\",..,,to\",.,..,\".and,.andto,,....\"\"\",andto,.and,,,,.toto.and\"and\",andand\"toto.,,.,to\".and\"to,.andandand.,,.\"to.and\",and,..and,,..,andtoto\"to.to,and.\"\".\",,,to.and..,\"and.,.\",,,toto.,..to.\".\".\"to.,,to.toto,.,,\".,,,,,,,,.,.to.,to,and\"andand.,,.to,andand,\",\",to,\".and\"\"to,.to.,,,,\"to,and,.\"\",..and\",..and\"and,,..,.to..,to,..,,and\".\"\"..\",\".to..,..,,.,,\",.,,,\"and,,,\"andto,\".and\"and,..andtoto,\",and,.\".andandto\".,,\".to...,,...and.,..andand\",to,and.\".andto\"and,\",,\",\",....and,.,,,.\",,.to,\"toand.\",,and.,,and\".to.,..to...\",.,\"..\"\",andand,,\",.,to...and\n"
     ]
    }
   ],
   "source": [
    "print(sample(model, 1000, first='Vision for the PlayStation Portable', top_k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a3d6b0-09c8-4d86-a3e4-3d1f2856e155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
